{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specific-adaptation",
   "metadata": {},
   "source": [
    "# Project Course - Foundations of Information Retrieval\n",
    "\n",
    "## Group 25\n",
    "\n",
    "## Contributors:\n",
    "- Elia Cunegatti e.cunegatti@student.utwente.nl \n",
    "- Ruben Popper r.popper@student.utwente.nl\n",
    "\n",
    "## Info:\n",
    "All descriptors (SIFT, SURF, HOG, ORB) and features representation (VGG-16, RESNET-34, RESNET-152) can be dowloaded at the following link : https://drive.google.com/drive/folders/1v9lAjoEzJQSnCGZ3U1nzyqeSt8jYcG_q?usp=sharing\n",
    "\n",
    "**Remark:** Please notice that the following Jupyter Notebook, although it allows to reproduce all the results contained in the report, it does not print all the results simultaneously. Due to the large computational time required to run all the experiment, we relied on different machines, notebooks and Google Collab sessions, as well as on a university-provided external server (Azure Machine), where the code was run with a classical .py script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic packages\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import tqdm\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Packages traditional CV algorithms \n",
    "import cv2\n",
    "'''\n",
    "please install opencv with the following pip command in order to use SIFT and SURF Algorithms\n",
    ">> pip install opencv-contrib-python==3.4.2.17\n",
    "'''\n",
    "import skimage\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import ORB, hog\n",
    "from skimage.transform import resize\n",
    "\n",
    "## Cluster packages\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "## Normalization and Distance packages\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial import distance\n",
    "\n",
    "## CNN packages\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-immunology",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load map images\n",
    "with open(\"data02/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "#load query images\n",
    "with open(\"data02/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "\n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"drive/MyDrive/data02/london_lite_gt.h5\",\"r\") as f:\n",
    "    fovs = f[\"fov\"][:]\n",
    "    sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-better",
   "metadata": {},
   "source": [
    "# Traditional CV Algorithms \n",
    "In the next cells, code is available to extract the descriptors with the Computer Vision algorithms we selected. \n",
    "Descriptors are also available in .bin files for each descriptor at the following link: https://drive.google.com/drive/folders/1v9lAjoEzJQSnCGZ3U1nzyqeSt8jYcG_q?usp=sharing.\n",
    "\n",
    "You can simply download them and upload them later without having to re-run all the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-religion",
   "metadata": {},
   "source": [
    "## ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_extractor = ORB(n_features=50)\n",
    "descriptors = None\n",
    "\n",
    "for img_m in m_imgs:\n",
    "    img = plt.imread(os.path.join('data02/', img_m))\n",
    "    img = rgb2gray(img)\n",
    "  \n",
    "    _,descriptors_img = descriptor_extractor.descriptors  \n",
    "    \n",
    "   \n",
    "    if descriptors is None:\n",
    "        descriptors = descriptors_img\n",
    "    else:\n",
    "        descriptors = np.vstack( (descriptors, descriptors_img))\n",
    "    break\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DESCRIPTORS\n",
    "#f = open('data02/ORB-descriptors-map.bin', 'wb')\n",
    "#data = pickle.dump(descriptors, f)\n",
    "#f.close()\n",
    "\n",
    "#LOAD DESCRIPTORS\n",
    "descriptor_extractor = ORB(n_features=50)\n",
    "f = open('data02/ORB-descriptors-map.bin', 'rb')\n",
    "descriptors = pickle.load(f)\n",
    "f.close()\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-nothing",
   "metadata": {},
   "source": [
    "## SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create(50)\n",
    "descriptors = None\n",
    "\n",
    "for img_m in m_imgs:\n",
    "    img = cv2.imread(os.path.join('data02/', img_m),cv2.IMREAD_GRAYSCALE)    \n",
    "    keypoints_sift, descriptors_img = sift.detectAndCompute(img, None)\n",
    "    if descriptors is None:\n",
    "        descriptors = descriptors_img\n",
    "    else:\n",
    "        descriptors = np.vstack((descriptors, descriptors_img))\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DESCRIPTORS\n",
    "#f = open('data02/SIFT-descriptors-map.bin', 'wb')\n",
    "#data = pickle.dump(descriptors, f)\n",
    "#f.close()\n",
    "\n",
    "#LOAD DESCRIPTORS\n",
    "sift = cv2.xfeatures2d.SIFT_create(50)\n",
    "f = open('data02/SIFT-descriptors-map.bin', 'rb')\n",
    "descriptors = pickle.load(f)\n",
    "f.close()\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-cleanup",
   "metadata": {},
   "source": [
    "## SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "descriptors = None\n",
    "\n",
    "for img_m in m_imgs:\n",
    "    img = cv2.imread(os.path.join('drive/MyDrive/data02/', img_m),cv2.IMREAD_GRAYSCALE)\n",
    "    _, descriptors_img = surf.detectAndCompute(img, None)\n",
    "    if descriptors is None:\n",
    "        descriptors = descriptors_img\n",
    "    else:\n",
    "        descriptors = np.vstack( (descriptors, descriptors_img))\n",
    "    break\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DESCRIPTORS\n",
    "#f = open('data02/SURF-descriptors-map.bin', 'wb')\n",
    "#data = pickle.dump(descriptors, f)\n",
    "#f.close()\n",
    "\n",
    "#LOAD DESCRIPTORS\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "f = open('data02/SURF-descriptors-map.bin', 'rb')\n",
    "descriptors = pickle.load(f)\n",
    "f.close()\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-perry",
   "metadata": {},
   "source": [
    "## HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = None\n",
    "for img_m in m_imgs:\n",
    "    img = plt.imread(os.path.join('drive/MyDrive/data02/', img_m))\n",
    "    resized_img = resize(img, (128*2, 64*2))\n",
    "    _, descriptors_img = hog(resized_img, orientations=9, pixels_per_cell=(8, 8), \n",
    "          cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "    if descriptors is None:\n",
    "        descriptors = descriptors_img\n",
    "    else:\n",
    "        descriptors = np.vstack( (descriptors, descriptors_img))\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DESCRIPTORS\n",
    "#f = open('data02/HOG-descriptors-map.bin', 'wb')\n",
    "#data = pickle.dump(descriptors, f)\n",
    "#f.close()\n",
    "\n",
    "#LOAD DESCRIPTORS\n",
    "f = open('data02/HOG-descriptors-map.bin', 'rb')\n",
    "descriptors = pickle.load(f)\n",
    "f.close()\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-lambda",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "First of all find the correct number of cluster to use based on your selected descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(random_state=42, n_init=2, verbose=1)\n",
    "K= [x for x in range(5,200,10)]\n",
    "visualizer = KElbowVisualizer(model, k= K, timings= True)\n",
    "visualizer.fit(descriptors)        \n",
    "visualizer.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-trainer",
   "metadata": {},
   "source": [
    "Once you find the correct value of K please change the variable k below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "# clustering\n",
    "\n",
    "algorithm = \"ORB\"\n",
    "algorithm = \"SIFT\"\n",
    "algorithm = \"SURF\"\n",
    "algorithm = \"HOG\"\n",
    "\n",
    "if algorithm = \"HOG\":\n",
    "    K = 55 \n",
    "else:\n",
    "    K = 45\n",
    "    \n",
    "num_initialization = 5 \n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=num_initialization, verbose=1)\n",
    "clusters = kmeans.fit(descriptors)\n",
    "centroids = clusters.cluster_centers_\n",
    "print(\"Shape of the centroids matrix: \", centroids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-creek",
   "metadata": {},
   "source": [
    "## BOW Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(centroids, img_descriptors, key):\n",
    "    n_centroids = centroids.shape[0] \n",
    "    n_descriptors = img_descriptors.shape[0]\n",
    "    bow_vector = np.zeros(n_centroids)  \n",
    "    for i in range(n_descriptors):\n",
    "        l = []\n",
    "        if key == 'cosine':\n",
    "        for j in range(n_centroids):\n",
    "            l.append(distance.cosine(img_descriptors[i], centroids[j]))\n",
    "        elif key = 'euclidean':\n",
    "            l.append(distance.euclidean(img_descriptors[i], centroids[j]))\n",
    "        elif key = 'minkowski':\n",
    "            l.append(distance.minkowski(img_descriptors[i], centroids[j]))\n",
    "        elif key = 'manhattan':\n",
    "            l.append(distance.cityblock(img_descriptors[i], centroids[j]))  \n",
    "        dist = min(l)\n",
    "        c = l.index(dist)\n",
    "        bow_vector[c] += 1\n",
    "\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-upgrade",
   "metadata": {},
   "source": [
    "Please now select a distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT YOUR DISTANCE!\n",
    "\n",
    "dist = 'cosine'\n",
    "#dist = 'euclidean'\n",
    "#dist = 'minkowski'\n",
    "#dist = 'manhattan'\n",
    "\n",
    "bow_map_images = None\n",
    "for img_name in tqdm(m_imgs):\n",
    "    \n",
    "    if algorithm == 'ORB':\n",
    "        img = plt.imread(os.path.join('drive/MyDrive/data02/', img_name))\n",
    "        img = rgb2gray(img)\n",
    "        descriptor_extractor.detect_and_extract(img)  \n",
    "         _,descriptors_img = descriptor_extractor.descriptors\n",
    "    \n",
    "    elif algorithm = 'SIFT':\n",
    "        img = cv2.imread(os.path.join('drive/MyDrive/data02/', img_name),cv2.IMREAD_GRAYSCALE)    \n",
    "        _, descriptors_img = sift.detectAndCompute(img, None)\n",
    "    \n",
    "    elif algorithm = 'SURF':\n",
    "        img = cv2.imread(os.path.join('drive/MyDrive/data02/', img_name),cv2.IMREAD_GRAYSCALE)    \n",
    "        _, descriptors_img = surf.detectAndCompute(img, None)\n",
    "    \n",
    "    elif algorithm = 'HOG':\n",
    "         img = plt.imread(os.path.join('drive/MyDrive/data02/',img_name))\n",
    "        resized_img = resize(img, (128*2, 64*2))\n",
    "\n",
    "        _, descriptors_img = hog(resized_img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                  cells_per_block=(2, 2), visualize=True, multichannel=True) \n",
    "    \n",
    "    bow = bag_of_words(centroids,descriptors_img, dist)\n",
    "    if bow_map_images is None:\n",
    "        bow_map_images = bow\n",
    "    else:\n",
    "        bow_map_images = np.vstack( (bow_map_images, bow))\n",
    "\n",
    "print(bow_map_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(bow_map_images)\n",
    "bow_map_images = scaler.transform(bow_map_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RETRIEVED IMAGES' SIMILARITIES\n",
    "\n",
    "def retrieve_images(map_bow_vectors, query_bow, key):\n",
    "    n_map_bow_vectors = map_bow_vectors.shape[0]\n",
    "    bow_distances = np.zeros(n_map_bow_vectors)\n",
    "    most_similar = None\n",
    "    l = []\n",
    "    index = []\n",
    "    for i in range(n_map_bow_vectors):\n",
    "        if key == 'cosine':\n",
    "            l.append(distance.cosine(query_bow,map_bow_vectors[i]))\n",
    "            index.append(i)\n",
    "        elif key == 'euclidian':\n",
    "            l.append(distance.euclidean(query_bow,map_bow_vectors[i]))\n",
    "            index.append(i)\n",
    "        elif key == 'minkowski':\n",
    "            l.append(distance.minkowski(query_bow,map_bow_vectors[i],p=1))\n",
    "            index.append(i)            \n",
    "        elif key == 'manhattan':\n",
    "            l.append(distance.cityblock(query_bow,map_bow_vectors[i]))\n",
    "            index.append(i)\n",
    "            \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame()\n",
    "    df[\"index\"] = index\n",
    "    df[\"value\"] = l\n",
    "    df = df.sort_values(by='value', ascending=True)\n",
    "    most_similar = list(df[\"index\"])\n",
    "    \n",
    "    return most_similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = []\n",
    "\n",
    "distance_all = [dist]\n",
    "retrieved_all = []\n",
    "relevant_all = []\n",
    "for item in distance_all:\n",
    "    retrieved = {}\n",
    "    relevant = {}\n",
    "    for i in range(len(q_imgs)):    \n",
    "        if algorithm == 'ORB':\n",
    "            img = plt.imread(os.path.join('/data02/', q_imgs[i]))\n",
    "            img = rgb2gray(img)\n",
    "            descriptor_extractor.detect_and_extract(img)  \n",
    "             _,descriptors_img = descriptor_extractor.descriptors\n",
    "\n",
    "        elif algorithm = 'SIFT':\n",
    "            img = cv2.imread(os.path.join('data02/', q_imgs[i]),cv2.IMREAD_GRAYSCALE)    \n",
    "            _, descriptors_img = sift.detectAndCompute(img, None)\n",
    "\n",
    "        elif algorithm = 'SURF':\n",
    "            img = cv2.imread(os.path.join('data02/', q_imgs[i]),cv2.IMREAD_GRAYSCALE)    \n",
    "            _, descriptors_img = surf.detectAndCompute(img, None)\n",
    "\n",
    "        elif algorithm = 'HOG':\n",
    "             img = plt.imread(os.path.join('data02/',q_imgs[i]))\n",
    "            resized_img = resize(img, (128*2, 64*2))\n",
    "\n",
    "            _, descriptors_img = hog(resized_img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                      cells_per_block=(2, 2), visualize=True, multichannel=True) \n",
    "    \n",
    "    \n",
    "        bow = bag_of_words(centroids, descriptors_img,dist)\n",
    "        bow = scaler.transform(bow.reshape(-1, 1).transpose())\n",
    "        bow = bow.transpose().reshape(-1)\n",
    "        retrieved_images = retrieve_images(bow_map_images, bow ,item)\n",
    "        relevant_images = np.where(sim[i, :] == 1)[0]\n",
    "        relevant[i] = list(relevant_images)\n",
    "        retrieved[i] = retrieved_images\n",
    "    retrieved_all.append(retrieved)\n",
    "    relevant_all.append(relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-beverage",
   "metadata": {},
   "source": [
    "## Measure performance Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(relevant, retrieved):\n",
    "    pp = []\n",
    "    for i in range(len(retrieved)):\n",
    "        ret = retrieved[:i+1]\n",
    "        if retrieved[i] in relevant:\n",
    "            numerator = list(set(relevant) & set(ret))\n",
    "            precision = len(numerator) / len(ret)\n",
    "            pp.append(precision)\n",
    "    k_sum = 0\n",
    "    for item in pp:\n",
    "        k_sum = k_sum + item\n",
    "    if len(pp) > 0:\n",
    "        return float(k_sum/len(pp)) \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    def mean_average_precision(all_relevant, all_retrieved):\n",
    "    pp = []    \n",
    "    for key, value in all_retrieved.items():\n",
    "        list_retrieved = list(value)        \n",
    "        list_relevant = all_relevant[key]\n",
    "        pp.append(average_precision(list_relevant, list_retrieved))\n",
    "    k_sum = 0\n",
    "    for item in pp:\n",
    "        k_sum = k_sum + item\n",
    "    return float(k_sum/len(pp))\n",
    "\n",
    "def mean_average_precision_at_k(all_relevant, all_retrieved,k):\n",
    "    pp = []\n",
    "    for key, value in all_retrieved.items():\n",
    "        list_retrieved = list(value)[:k]  \n",
    "        list_relevant = all_relevant[key]\n",
    "        pp.append(average_precision(list_relevant, list_retrieved))\n",
    "    k_sum = 0\n",
    "    for item in pp:\n",
    "        k_sum = k_sum + item\n",
    "    return float(k_sum/len(pp))\n",
    "\n",
    "\n",
    "def top_recall_at_k(all_relevant, all_retrieved,k):\n",
    "    numerator = 0\n",
    "    for i in range(len(all_relevant)):\n",
    "        if len(list(set.intersection(set(all_relevant[i]), set(all_retrieved[i][:k])))) > 0:\n",
    "            numerator += 1  \n",
    "    return numerator/len(all_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-jamaica",
   "metadata": {},
   "source": [
    "## Print Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for item in distance_all:\n",
    "    print('Distance Metric --> {}'.format(item))\n",
    "    mapr = mean_average_precision(relevant_all[i],retrieved_all[i])\n",
    "    print('Mean Average Precision (MAP) with distance {0} --> {1}'.format(item,round(mapr,3)))\n",
    "    k_list = [1,5,10,50,100,200]\n",
    "    for item in k_list:\n",
    "        top = top_recall_at_k(relevant_all[i],retrieved_all[i],item)\n",
    "        print('Top-Recall-at-k with k {0} --> {2}'.format(item,round(top,3)))\n",
    "        mapr = mean_average_precision_at_k(relevant_all[i],retrieved_all[i],item)\n",
    "        print('MAP-at-k {0} --> {1}'.format(item,round(mapr,3)))\n",
    "    print(\"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-formation",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Please comment/uncomment the lines below in order to select one of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'vgg-16'\n",
    "#model = 'resnet-34'\n",
    "#model = 'resnet-152'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'vgg-16':\n",
    "    class FeatureExtractor(nn.Module):\n",
    "          def __init__(self, model):\n",
    "            super(FeatureExtractor, self).__init__()\n",
    "            self.features = list(model.features)\n",
    "            self.features = nn.Sequential(*self.features)\n",
    "            self.pooling = model.avgpool\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc = model.classifier[0]\n",
    "\n",
    "            def forward(self, x):\n",
    "                out = self.features(x)\n",
    "                out = self.pooling(out)\n",
    "                out = self.flatten(out)\n",
    "                out = self.fc(out) \n",
    "                return out \n",
    "   \n",
    "    model = models.vgg16(pretrained=True)\n",
    "    new_model = FeatureExtractor(model)\n",
    "    device = torch.device(\"cuda\")\n",
    "    new_model = new_model.to(device)\n",
    "\n",
    "elif model == 'resnet-34':\n",
    "    model = models.resnet34(pretrained=True)\n",
    "    new_model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    new_model = new_model.to(device)\n",
    "    \n",
    "elif model == 'resnet-152':\n",
    "    model = models.resnet152(pretrained=True)\n",
    "    new_model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "    device = torch.device(\"cuda\")\n",
    "    new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-highland",
   "metadata": {},
   "source": [
    "You can skip this cell if you want to use the features already extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.CenterCrop(512),\n",
    "  transforms.Resize(448),\n",
    "  transforms.ToTensor()                              \n",
    "])\n",
    "\n",
    "features = []\n",
    "\n",
    "for img_m in m_imgs:\n",
    "    img = cv2.imread(os.path.join('drive/MyDrive/data02/', img_m),cv2.IMREAD_UNCHANGED)\n",
    "    img = transform(img)\n",
    "    img = img.reshape(1, 3, 448, 448)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        feature = new_model(img)\n",
    "    features.append(feature.cpu().detach().numpy().reshape(-1))\n",
    "\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE DESCRIPTORS\n",
    "#f = open('data02/Model-descriptors-map.bin', 'wb')\n",
    "#data = pickle.dump(descriptors, f)\n",
    "#f.close()\n",
    "\n",
    "#LOAD DESCRIPTORS\n",
    "if model == 'vgg-16':\n",
    "    f = open('data02/VGG-16-descriptors-map.bin', 'rb')\n",
    "    descriptors = pickle.load(f)\n",
    "    f.close()\n",
    "elif model == 'resnet-34':\n",
    "    f = open('data02/RESNET-34-descriptors-map.bin', 'rb')\n",
    "    descriptors = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "elif model == 'resnet-152':\n",
    "    f = open('data02/RESNET-152-descriptors-map.bin', 'rb')\n",
    "    descriptors = pickle.load(f)\n",
    "    f.close()\n",
    "print(descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(features)\n",
    "features = scaler.transform(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_all = [\"cosine\",\"minkowski\",\"euclidian\",\"manhattan\"]\n",
    "retrieved_all = []\n",
    "relevant_all = []\n",
    "for item in distance_all:\n",
    "    retrieved = {}\n",
    "    relevant = {}\n",
    "    for i in range(0,500):\n",
    "        img = cv2.imread(os.path.join('drive/MyDrive/data02/',q_imgs[i]),cv2.IMREAD_UNCHANGED)\n",
    "        img = transform(img)\n",
    "        img = img.reshape(1, 3, 448, 448)\n",
    "        img = img.to(device)\n",
    "        with torch.no_grad():\n",
    "            img_feature = new_model(img)\n",
    "\n",
    "        img_feature = (img_feature.cpu().detach().numpy().reshape(-1))\n",
    "        img_feature = np.array(img_feature)\n",
    "        \n",
    "        img_feature = scaler.transform(img_feature.reshape(-1, 1).transpose())\n",
    "        img_feature = img_feature.transpose().reshape(-1)\n",
    "        retrieved_images = retrieve_images(features,img_feature,item)\n",
    "        relevant_images = np.where(sim[i, :] == 1)[0]\n",
    "        relevant[i] = list(relevant_images)\n",
    "        retrieved[i] = retrieved_images\n",
    "    retrieved_all.append(retrieved)\n",
    "    relevant_all.append(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for item in distance_all:\n",
    "    print('Distance Metric --> {}'.format(item))\n",
    "    mapr = mean_average_precision(relevant_all[i],retrieved_all[i])\n",
    "    print('Mean Average Precision (MAP) with distance {0} --> {1}'.format(item,round(mapr,3)))\n",
    "    k_list = [1,5,10,50,100,200]\n",
    "    for item in k_list:\n",
    "        top = top_recall_at_k(relevant_all[i],retrieved_all[i],item)\n",
    "        print('Top-Recall-at-k with k {0} --> {2}'.format(item,round(top,3)))\n",
    "        mapr = mean_average_precision_at_k(relevant_all[i],retrieved_all[i],item)\n",
    "        print('MAP-at-k {0} --> {1}'.format(item,round(mapr,3)))\n",
    "    print(\"\\n\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
